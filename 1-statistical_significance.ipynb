{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing\n",
    "\n",
    "__The objective in this notebook is to demonstrate different forms of statistical tests on recorded metrics of a hypothetical A/B test to see if there is a statistical difference between the two groups.__\n",
    "\n",
    "\n",
    "_Hypthetical experiment setup:_ We've collected data for a web-based experiment. We're testing a layout change to see if this affects the proportion of people who click on a button to go to the download page. This experiment is designed to have a cookie-based diversion, and we record two things from each user: \n",
    "- which page version they received\n",
    "- whether or not they accessed the download page during the data recording period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "# from statsmodels.stats.weightstats import ztest\n",
    "# from statsmodels.stats import proportion as proptests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition  click\n",
       "0          1      0\n",
       "1          0      0\n",
       "2          0      0\n",
       "3          1      1\n",
       "4          1      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(999, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import data\n",
    "data = pd.read_csv('data/statistical_significance_data.csv')\n",
    "display(data.head(5))\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ In the dataset, the 'condition' column takes a 0 for the control group, and 1 for the experimental group. The 'click' column takes a values of 0 for no click, and 1 for a click. There are 999 samples in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participants</th>\n",
       "      <th>clickrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>508</td>\n",
       "      <td>0.112205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>0.079430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participants  clickrate\n",
       "1           508   0.112205\n",
       "0           491   0.079430"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count participants per group and calculate mean click rates\n",
    "df = pd.DataFrame()\n",
    "df['participants'] = data['condition'].value_counts()\n",
    "df['clickrate']= data.groupby('condition').mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking the Invariant Metric\n",
    "\n",
    "**Invariant metrics**: Metrics that we hope will not be different between groups. Metrics in this category serve to check that the experiment is running as expected. \n",
    "\n",
    "In this case, we should check that the number of visitors assigned to each group is similar. We want to do a _two-sided hypothesis test_ on the proportion of visitors assigned to one of our conditions. \n",
    "\n",
    "There are two main approaches for that:\n",
    "\n",
    "1. __Simulation-based approach:__ We can simulate the number of visitors that would be assigned to each group for the number of total observations, assuming that we have an expected 50/50 split (200'000 repetitions should provide a good speed-variability balance in this case) and then see in how many simulated cases we get as extreme or more extreme a deviation from 50/50 that we actually observed. The proportion of flagged simulation outcomes gives us a p-value on which to assess our observed proportion. We hope to see a larger p-value, insufficient evidence to reject the null hypothesis.\n",
    "\n",
    "2. __Analytic approach:__ We could use the exact binomial distribution to compute a p-value for the test. The more usual approach, however, is to use the normal distribution approximation. (This is possible thanks to our large sample size and the central limit theorem). Because we appoximate a discrete distribution by a continuous distribution, we should perform a [continuity correction](https://en.wikipedia.org/wiki/Continuity_correction), to get a precise p-value. This means either adding or subtracting 0.5 to the total count before computing the area underneath the curve. (e.g. If we had 415 / 850 assigned to the control group, then the normal approximation would take the area to the left of $(415 + 0.5) / 850 = 0.489$ and to the right of $(435 - 0.5) / 850 = 0.511$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of trials and number of 'successes'\n",
    "n_obs = data.shape[0]\n",
    "n_control = data.groupby('condition').size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:  0.612905\n"
     ]
    }
   ],
   "source": [
    "# simulate outcomes under null, compare to observed outcome\n",
    "p = 0.5\n",
    "n_trials = 200_000\n",
    "\n",
    "samples = np.random.binomial(n_obs, p, n_trials)\n",
    "\n",
    "\n",
    "print(\"p-value: \", (np.logical_or(samples <= n_control, samples >= (n_obs - n_control)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic approach\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score:  -0.5062175977346661\n",
      "p-value:  0.6127039025537114\n"
     ]
    }
   ],
   "source": [
    "# Compute a z-score and p-value\n",
    "p = 0.5\n",
    "sd = np.sqrt(p * (1-p) * n_obs)\n",
    "z = ((n_control + 0.5) - p * n_obs) / sd\n",
    "\n",
    "print(\"z-score: \", z)\n",
    "print(\"p-value: \", 2 * stats.norm.cdf(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-246f470fe6ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n_obs' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 300 / 2000\n",
    "sd = np.sqrt(p * (1-p) * n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** The p-value is around .613. Since the difference between the groups isn't statistically significant, we can move on to the test on the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking the Evaluation Metric\n",
    "\n",
    "__Evaluation metrics:__ The metrics by which we compare groups. Ideally, we hope to see a difference between groups that will tell us if our manipulation was a success.\n",
    "\n",
    "Evaluation metric in this case: The click-through rate. We want to see that the experimental group has a significantly larger click-through rate than the control group. This is a _one-tailed test._\n",
    "\n",
    "\n",
    "Notes on the approaches:\n",
    "\n",
    "1. __The simulation approach__ for this metric isn't too different from the approach for the invariant metric. You'll need the overall click-through rate as the common proportion to draw simulated values from for each group. You may also want to perform more simulations since there's higher variance for this test.\n",
    "\n",
    "2. __Analytic approaches:__ We can make use of the normal approximation again. In addition to the pooled click-through rate, we'll need a pooled standard deviation in order to compute a _z-score_. (While there is a continuity correction possible in this case as well, it's much more conservative than the p-value that a simulation will usually imply. Computing the z-score and resulting p-value without a continuity correction should be closer to the simulation's outcomes, though slightly more optimistic about there being a statistical difference between groups.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in ctr:  0.03277498917523293\n"
     ]
    }
   ],
   "source": [
    "# get number of trials and overall 'success' rate under null\n",
    "n_control = data.groupby('condition').size()[0]\n",
    "n_exper = data.groupby('condition').size()[1]\n",
    "rate_null = data['click'].mean()\n",
    "\n",
    "rate_control = data.groupby('condition').mean().iloc[0]\n",
    "rate_exper = data.groupby('condition').mean().iloc[1]\n",
    "rate_diff = float(rate_exper - rate_control)\n",
    "\n",
    "# check\n",
    "print(\"Difference in ctr: \", rate_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation-based approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:  0.039305\n"
     ]
    }
   ],
   "source": [
    "# simulate outcomes under null, compare to observed outcome\n",
    "n_trials = 200_000\n",
    "\n",
    "control_clicks = np.random.binomial(n_control, rate_null, n_trials)\n",
    "exper_clicks = np.random.binomial(n_exper, rate_null, n_trials)\n",
    "samples = (exper_clicks / n_exper) - (control_clicks / n_control)\n",
    "\n",
    "print(\"p-value: \", (samples >= (rate_diff)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score:  1.7571887396196666\n",
      "p-value:  0.039442821974613684\n"
     ]
    }
   ],
   "source": [
    "# compute standard error, z-score, and p-value\n",
    "se_p = np.sqrt(rate_null * (1-rate_null) * (1/n_control + 1/n_exper))\n",
    "\n",
    "z = (rate_diff) / se_p\n",
    "print(\"z-score: \", z)\n",
    "print(\"p-value: \", 1-stats.norm.cdf(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11220472440944881"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['click'][data['condition'] == 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** The p-value at around .039 indicates a statistically significant difference at an alpha = .05 level, so we can conclude that the experiment had the desired effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
